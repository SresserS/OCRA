<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="OCRA: Object-Centric Learning with 3D and Tactile Priors for Human-to-Robot Action Transfer">
    <meta name="keywords" content="Robotics, ICRA">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>OCRA: Object-Centric Learning with 3D and Tactile Priors for Human-to-Robot Action Transfer</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">OCRA: Object-Centric Learning with 3D and Tactile Priors for Human-to-Robot Action Transfer</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                            <a href="https://sressers.github.io/knw/" target="_blank">Kuanning Wang</a><sup>1*</sup>, 
                            <a href="https://scholar.google.com.sg/citations?hl=en&user=426Vf3kAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Ke Fan</a><sup>1*</sup>, 
                            <a href="https://lovelyqian.github.io/" target="_blank">Yuqian Fu</a><sup>2</sup>, 
                            <a href="#" target="_blank">Siyu Lin</a><sup>1</sup>, 
                            <a href="#" target="_blank">Hu Luo</a><sup>1</sup>
                                </span>
                            <span class="author-block">
                            <a href="https://danielseita.github.io/" target="_blank">Daniel Seita</a><sup>3</sup>, 
                            <a href="http://yanweifu.github.io/" target="_blank">Yanwei Fu</a><sup>1</sup>, 
                            <a href="https://teai.fudan.edu.cn/" target="_blank">Yu-Gang Jiang</a><sup>1</sup>, 
                            <a href="https://scholar.google.com/citations?user=DTbhX6oAAAAJ&hl=en" target="_blank">Xiangyang Xue</a><sup>1</sup>
                        </span>
                        </div>
                        <div class="is-size-5 publication-affiliations mt-2">
                            <span class="affiliation-block"><sup>1</sup> Fudan University</span>
                            <span class="affiliation-block">&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup> INSAIT, Sofia University</span>
                            <span class="affiliation-block">&nbsp;&nbsp;&nbsp;&nbsp;<sup>3</sup> University of Southern California</span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">International Conference on Robotics and Automation <strong>(ICRA)</strong> 2026</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- Paper links can be added here when available -->
                                <!-- 
                                <span class="link-block">
                                    <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                -->
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Video Section -->
    <section class="hero is-light is-small">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <div class="content">
                <video id="video1" controls width="100%" style="border-radius: 10px;">
                  <source src="./static/videos/ocra.mp4"
                          type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                We present OCRA, an object-centric framework for video-based human-to-robot action transfer that learns directly from human demonstration videos to enable robust manipulation. Object-centric learning emphasizes task-relevant objects and their interactions while filtering out irrelevant background, providing a natural and scalable way to teach robots. OCRA leverages multi-view RGB videos, the state-of-the-art 3D foundation model VGGT, and advanced detection and segmentation models to reconstruct object-centric 3D point clouds, capturing rich interactions between objects. To handle properties not easily perceived by vision alone, we incorporate tactile priors via a large-scale dataset of over one million tactile images. These 3D and tactile priors are fused through a multimodal module (ResFiLM) and fed into a Diffusion Policy to generate robust manipulation actions. Extensive experiments on both vision-only and visuo-tactile tasks show that OCRA significantly outperforms existing baselines and ablations, demonstrating its effectiveness for learning from human demonstration videos.
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
    
      </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h3 class="title is-3">Method</h3>
                    <div class="content has-text-justified">
                        <!-- Add framework image here when available -->
                        
                        <img src="static/images/framework.png" alt="Method" style="max-width: 100%; height: auto; margin-top: 20px;">
                       
                        <p>
                            The left column illustrates our human demonstration collection system. Two RGB cameras capture demonstration videos, while the blue box highlights a portable tactile gripper for tactile data collection, which also records fingertip tactile images used to build our large-scale tactile dataset (shown at the bottom).

                            The first row depicts how OCRA processes multi-view RGB inputs to obtain object-centric 3D priors. We first reconstruct the 3D scene using VGGT, followed by bi-view metric depth prediction for world-scale alignment. GroundingDINO and SAM2 then provide object segmentation masks, divided into a Manipulable Object Mask (for target objects) and a Context Object Mask (for surrounding objects). These are used to extract visual object-centric representations across modalities (segmentation, point cloud).
                            
                            The middle of the second row shows tactile-prior extraction via Tactile Encoder pretraining under a Masked Autoencoder paradigm.
                            
                            The right of the second row presents policy deployment. Multi-view RGB and tactile images are encoded into geometric and tactile features, which are fused by ResFiLM and passed to a Diffusion Policy. The policy predicts actions through iterative denoising of noisy action samples.
                            
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

            <footer class="footer">
              <div class="container">
                  <div class="columns is-centered">
                      <div class="column is-8">
                          <div class="content">
                              <p>
                                  
                              </p>
                          </div>
                      </div>
                  </div>
              </div>
          </footer>


</body>

</html>

